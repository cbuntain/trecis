{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import gzip\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy import interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.cluster\n",
    "import sklearn.feature_extraction \n",
    "import sklearn.feature_extraction.text\n",
    "import sklearn.metrics\n",
    "import sklearn.preprocessing\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.semi_supervised import LabelPropagation\n",
    "from sklearn.semi_supervised import LabelSpreading\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/clb617/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clb617/anaconda3/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as VS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tweet Count: 25923\n"
     ]
    }
   ],
   "source": [
    "tweet_id_map = {}\n",
    "with open(\"../data/2018-testing.json\", \"r\") as in_file:\n",
    "    for line in in_file:\n",
    "        tweet = json.loads(line)\n",
    "        tweet_id_map[np.int64(tweet[\"id\"])] = tweet\n",
    "        \n",
    "with open(\"../data/2018-training.json\", \"r\") as in_file:\n",
    "    for line in in_file:\n",
    "        tweet = json.loads(line)\n",
    "        tweet_id_map[np.int64(tweet[\"id\"])] = tweet\n",
    "\n",
    "print(\"Total Tweet Count:\", len(tweet_id_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 25674,\n",
       " 'es': 57,\n",
       " 'tl': 97,\n",
       " 'it': 3,\n",
       " 'fr': 13,\n",
       " 'pt': 9,\n",
       " 'und': 42,\n",
       " 'hi': 9,\n",
       " 'nl': 3,\n",
       " 'ca': 2,\n",
       " 'eu': 1,\n",
       " 'in': 2,\n",
       " 'ro': 3,\n",
       " 'et': 1,\n",
       " 'de': 1,\n",
       " 'cs': 1,\n",
       " 'ht': 1,\n",
       " 'ja': 3,\n",
       " 'th': 1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_count_map = {}\n",
    "for lang in [tweet[\"lang\"] for tweet in tweet_id_map.values()]:\n",
    "    lang_count_map[lang] = lang_count_map.get(lang, 0) + 1\n",
    "lang_count_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_priority_map = {}\n",
    "priority_df = pd.read_csv(\"tweet_to_priority.csv\", dtype={\"tweet_id\": np.int64})\n",
    "for row in priority_df.itertuples():\n",
    "    tweet_id = row.tweet_id\n",
    "    \n",
    "    tweet_priority_map[tweet_id] = {\n",
    "        \"score\": row.score_mean,\n",
    "        \"weight\": 1.0 - row.score_std\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_category_map = {}\n",
    "category_df = pd.read_csv(\"tweet_to_category.csv\")\n",
    "for category, tweets in category_df.groupby(\"category\"):\n",
    "    tweet_category_map[category] = list(tweets[\"tweet_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: 44849\n"
     ]
    }
   ],
   "source": [
    "print(\"Labels:\", sum([len(v) for v in tweet_category_map.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: Advice\n",
      "\tTweet Count: 1236 Retrieved Fraction: 1.0\n",
      "\t {'en': 1235, 'und': 1}\n",
      "Category: CleanUp\n",
      "\tTweet Count: 61 Retrieved Fraction: 1.0\n",
      "\t {'en': 61}\n",
      "Category: ContinuingNews\n",
      "\tTweet Count: 5286 Retrieved Fraction: 1.0\n",
      "\t {'en': 5275, 'ja': 1, 'tl': 4, 'und': 2, 'hi': 1, 'fr': 1, 'es': 1, 'pt': 1}\n",
      "Category: Discussion\n",
      "\tTweet Count: 2241 Retrieved Fraction: 1.0\n",
      "\t {'en': 2199, 'und': 5, 'hi': 1, 'es': 27, 'pt': 3, 'it': 1, 'fr': 2, 'tl': 1, 'nl': 2}\n",
      "Category: Donations\n",
      "\tTweet Count: 811 Retrieved Fraction: 1.0\n",
      "\t {'en': 811}\n",
      "Category: EmergingThreats\n",
      "\tTweet Count: 732 Retrieved Fraction: 1.0\n",
      "\t {'en': 731, 'fr': 1}\n",
      "Category: Factoid\n",
      "\tTweet Count: 2502 Retrieved Fraction: 1.0\n",
      "\t {'en': 2496, 'tl': 4, 'und': 1, 'ca': 1}\n",
      "Category: FirstPartyObservation\n",
      "\tTweet Count: 3712 Retrieved Fraction: 1.0\n",
      "\t {'en': 3707, 'hi': 2, 'und': 2, 'tl': 1}\n",
      "Category: GoodsServices\n",
      "\tTweet Count: 126 Retrieved Fraction: 1.0\n",
      "\t {'en': 126}\n",
      "Category: Hashtags\n",
      "\tTweet Count: 3261 Retrieved Fraction: 1.0\n",
      "\t {'en': 3243, 'es': 9, 'it': 1, 'fr': 2, 'und': 2, 'ca': 1, 'eu': 1, 'pt': 1, 'hi': 1}\n",
      "Category: InformationWanted\n",
      "\tTweet Count: 184 Retrieved Fraction: 1.0\n",
      "\t {'en': 184}\n",
      "Category: Irrelevant\n",
      "\tTweet Count: 2852 Retrieved Fraction: 1.0\n",
      "\t {'en': 2814, 'und': 5, 'pt': 3, 'in': 1, 'es': 6, 'tl': 19, 'fr': 1, 'nl': 1, 'hi': 2}\n",
      "Category: KnownAlready\n",
      "\tTweet Count: 1216 Retrieved Fraction: 1.0\n",
      "\t {'en': 1180, 'ja': 1, 'hi': 1, 'es': 28, 'it': 1, 'pt': 1, 'tl': 1, 'fr': 2, 'und': 1}\n",
      "Category: MovePeople\n",
      "\tTweet Count: 51 Retrieved Fraction: 1.0\n",
      "\t {'en': 51}\n",
      "Category: MultimediaShare\n",
      "\tTweet Count: 4080 Retrieved Fraction: 1.0\n",
      "\t {'en': 4036, 'tl': 1, 'und': 4, 'hi': 1, 'es': 33, 'it': 2, 'fr': 2, 'pt': 1}\n",
      "Category: Official\n",
      "\tTweet Count: 459 Retrieved Fraction: 1.0\n",
      "\t {'en': 457, 'fr': 1, 'pt': 1}\n",
      "Category: PastNews\n",
      "\tTweet Count: 1362 Retrieved Fraction: 1.0\n",
      "\t {'en': 1313, 'hi': 1, 'es': 39, 'pt': 2, 'it': 2, 'fr': 2, 'und': 2, 'tl': 1}\n",
      "Category: SearchAndRescue\n",
      "\tTweet Count: 298 Retrieved Fraction: 1.0\n",
      "\t {'en': 298}\n",
      "Category: Sentiment\n",
      "\tTweet Count: 6997 Retrieved Fraction: 1.0\n",
      "\t {'en': 6974, 'tl': 2, 'und': 16, 'hi': 3, 'es': 1, 'eu': 1}\n",
      "Category: ServiceAvailable\n",
      "\tTweet Count: 1127 Retrieved Fraction: 1.0\n",
      "\t {'en': 1123, 'hi': 2, 'ro': 2}\n",
      "Category: SignificantEventChange\n",
      "\tTweet Count: 451 Retrieved Fraction: 1.0\n",
      "\t {'en': 445, 'tl': 4, 'et': 2}\n",
      "Category: ThirdPartyObservation\n",
      "\tTweet Count: 4204 Retrieved Fraction: 1.0\n",
      "\t {'en': 4201, 'ca': 1, 'tl': 1, 'pt': 1}\n",
      "Category: Unknown\n",
      "\tTweet Count: 104 Retrieved Fraction: 1.0\n",
      "\t {'en': 101, 'tl': 1, 'nl': 2}\n",
      "Category: Volunteer\n",
      "\tTweet Count: 118 Retrieved Fraction: 1.0\n",
      "\t {'en': 118}\n",
      "Category: Weather\n",
      "\tTweet Count: 1378 Retrieved Fraction: 1.0\n",
      "\t {'en': 1377, 'tl': 1}\n"
     ]
    }
   ],
   "source": [
    "for category, tweet_ids in tweet_category_map.items():\n",
    "    retrieved_count = sum([1 if np.int64(tid) in tweet_id_map else 0 in tweet_id_map for tid in tweet_ids])\n",
    "    print(\"Category:\", category)\n",
    "    print(\"\\tTweet Count:\", len(tweet_ids), \"Retrieved Fraction:\", retrieved_count/len(tweet_ids))\n",
    "    \n",
    "    lang_count_map = {}\n",
    "    for lang in [tweet_id_map[np.int64(tid)][\"lang\"] for tid in tweet_ids if np.int64(tid) in tweet_id_map]:\n",
    "        lang_count_map[lang] = lang_count_map.get(lang, 0) + 1\n",
    "    print(\"\\t\", str(lang_count_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip stop words, retweet signs, @ symbols, and URL headers\n",
    "stopList = [\"http\", \"https\", \"rt\", \"@\", \":\", \"t.co\", \"co\", \"amp\", \"&amp;\", \"...\", \"\\n\", \"\\r\"]\n",
    "stopList.extend(string.punctuation)\n",
    "# stopList.extend(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenizer_wrapper(text):\n",
    "#     return [t.lemma_ for t in nlp(text)]\n",
    "\n",
    "local_tokenizer = TweetTokenizer()\n",
    "def tokenizer_wrapper(text):\n",
    "    return local_tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Additional Features\n",
    "sentiment_analyzer = VS()\n",
    "\n",
    "def count_twitter_objs(text_string):\n",
    "    \"\"\"\n",
    "    Accepts a text string and replaces:\n",
    "    1) urls with URLHERE\n",
    "    2) lots of whitespace with one instance\n",
    "    3) mentions with MENTIONHERE\n",
    "    4) hashtags with HASHTAGHERE\n",
    "\n",
    "    This allows us to get standardized counts of urls and mentions\n",
    "    Without caring about specific people mentioned.\n",
    "    \n",
    "    Returns counts of urls, mentions, and hashtags.\n",
    "    \"\"\"\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    hashtag_regex = '#[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, 'URLHERE', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, 'MENTIONHERE', parsed_text)\n",
    "    parsed_text = re.sub(hashtag_regex, 'HASHTAGHERE', parsed_text)\n",
    "    return(parsed_text.count('URLHERE'),parsed_text.count('MENTIONHERE'),parsed_text.count('HASHTAGHERE'))\n",
    "\n",
    "## Taken from Davidson et al.\n",
    "def other_features(tweet):\n",
    "    \"\"\"This function takes a string and returns a list of features.\n",
    "    These include Sentiment scores, Text and Readability scores,\n",
    "    as well as Twitter specific features\"\"\"\n",
    "    tweet_text = tweet[\"text\"]\n",
    "    \n",
    "    ##SENTIMENT\n",
    "    sentiment = sentiment_analyzer.polarity_scores(tweet_text)\n",
    "    \n",
    "    words = local_tokenizer.tokenize(tweet_text) #Get text only\n",
    "    \n",
    "    num_chars = sum(len(w) for w in words) #num chars in words\n",
    "    num_chars_total = len(tweet_text)\n",
    "    num_terms = len(tweet_text.split())\n",
    "    num_words = len(words)\n",
    "    num_unique_terms = len(set([x.lower() for x in words]))\n",
    "    \n",
    "    caps_count = sum([1 if x.isupper() else 0 for x in tweet_text])\n",
    "    caps_ratio = caps_count / num_chars_total\n",
    "    \n",
    "    twitter_objs = count_twitter_objs(tweet_text) #Count #, @, and http://\n",
    "    num_media = 0\n",
    "    if \"entities\" in tweet and \"media\" in tweet[\"entities\"]:\n",
    "        num_media = len(tweet[\"entities\"][\"media\"])\n",
    "    retweet = 0\n",
    "    if \"rt\" in words or \"retweeted_status\" in tweet:\n",
    "        retweet = 1\n",
    "        \n",
    "    has_place = 1 if \"coordinates\" in tweet else 0\n",
    "        \n",
    "    author = tweet[\"user\"]\n",
    "    is_verified = 1 if (\"verified\" in author and author[\"verified\"]) else 0\n",
    "    log_followers = 0\n",
    "    if \"followers_count\" in author and author[\"followers_count\"] > 0:\n",
    "         log_followers = np.log(author[\"followers_count\"])\n",
    "    log_friends = 0\n",
    "    if \"friends_count\" in author and author[\"friends_count\"] > 0:\n",
    "         log_followers = np.log(author[\"friends_count\"])\n",
    "    \n",
    "    features = [num_chars, num_chars_total, num_terms, num_words,\n",
    "                num_unique_terms, sentiment['neg'], sentiment['pos'], \n",
    "                sentiment['neu'], sentiment['compound'],\n",
    "                twitter_objs[2], twitter_objs[1],\n",
    "                twitter_objs[0], retweet, num_media,\n",
    "                is_verified, \n",
    "#                 log_followers, log_friends,\n",
    "#                 has_place,\n",
    "                caps_ratio,\n",
    "               ]\n",
    "\n",
    "    return [round(x, 4) for x in features]\n",
    "\n",
    "other_features_names = [\"num_chars\", \"num_chars_total\", \\\n",
    "                        \"num_terms\", \"num_words\", \"num_unique_words\", \"vader neg\",\"vader pos\",\n",
    "                        \"vader neu\", \"vader compound\", \\\n",
    "                        \"num_hashtags\", \"num_mentions\", \n",
    "                        \"num_urls\", \"is_retweet\", \"num_media\",\n",
    "                        \"is_verified\", \n",
    "#                         \"log_followers\", \"log_friends\",\n",
    "#                         \"has_place\",\n",
    "                        \"caps_ratio\",\n",
    "                       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(\n",
    "    tokenizer=tokenizer_wrapper,\n",
    "    ngram_range=(1, 1),\n",
    "    stop_words=stopList, #We do better when we keep stopwords\n",
    "    use_idf=True,\n",
    "    smooth_idf=False,\n",
    "    norm=None, #Applies l2 norm smoothing\n",
    "    decode_error='replace',\n",
    "    max_features=10000,\n",
    "    min_df=4,\n",
    "    max_df=0.501\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_to_label = {c:i for i, c in enumerate(tweet_category_map.keys()) if c != \"Irrelevant\"}\n",
    "category_to_label[\"Irrelevant\"] = -1\n",
    "\n",
    "tweet_id_to_category = {}\n",
    "for category, tweet_ids in tweet_category_map.items():\n",
    "    if ( len(tweet_ids) < 5 ):\n",
    "        print(\"Skipping category:\", category)\n",
    "        continue\n",
    "        \n",
    "    for tweet_id in tweet_ids:\n",
    "        tweet_id_to_category[np.int64(tweet_id)] = category_to_label[category]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet_pairs = [(tweet, tweet_id_to_category[tid]) \n",
    "#                for tid, tweet in tweet_id_map.items() if tid in tweet_id_to_category]\n",
    "\n",
    "tweet_pairs = [(tweet_id_map[np.int64(tweet)], category_to_label[category]) \n",
    "               for category, tweet_ids in tweet_category_map.items() for tweet in tweet_ids]\n",
    "\n",
    "tweet_texts = [tp[0][\"text\"] for tp in tweet_pairs]\n",
    "\n",
    "y_data = np.array([tp[1] for tp in tweet_pairs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples: 44849 44849\n"
     ]
    }
   ],
   "source": [
    "print(\"Samples:\", len(tweet_pairs), len(y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(s):\n",
    "    \"\"\"\n",
    "    Given a text, cleans and normalizes it. Feel free to add your own stuff.\n",
    "    From: https://www.kaggle.com/mschumacher/using-fasttext-models-for-robust-embeddings\n",
    "    \"\"\"\n",
    "    s = s.lower()\n",
    "\n",
    "    # Replace numbers and symbols with language\n",
    "    s = s.replace('&', ' and ')\n",
    "    s = s.replace('@', ' at ')\n",
    "    s = s.replace('0', 'zero')\n",
    "    s = s.replace('1', 'one')\n",
    "    s = s.replace('2', 'two')\n",
    "    s = s.replace('3', 'three')\n",
    "    s = s.replace('4', 'four')\n",
    "    s = s.replace('5', 'five')\n",
    "    s = s.replace('6', 'six')\n",
    "    s = s.replace('7', 'seven')\n",
    "    s = s.replace('8', 'eight')\n",
    "    s = s.replace('9', 'nine')\n",
    "\n",
    "    return s\n",
    "\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "def ft_tokenizer(text):\n",
    "    return [normalize(t) for t in analyzer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_gensim = FastText.load('text_sample_2015_gensim.model')\n",
    "model_gensim = FastText.load('../models/text_sample_2013to2016_gensim_200.model')\n",
    "# model_gensim = FastText.load_fasttext_format('../data/cc.en.300.bin')\n",
    "wvs = model_gensim.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(sentence):\n",
    "    tokenized = [normalize(t) for t in analyzer(sentence)]\n",
    "    \n",
    "    wv_vecs = []\n",
    "    for t in tokenized:\n",
    "\n",
    "        try:\n",
    "            v = wvs[t]\n",
    "            norm = np.linalg.norm(v)\n",
    "            normed_v = (v / norm)\n",
    "            wv_vecs.append(normed_v)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    m = np.array(wv_vecs)\n",
    "    normed_m = np.mean(m, axis=0)\n",
    "\n",
    "    return normed_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44849, 200)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_features_ = [vectorize(s) for s in tweet_texts]\n",
    "ft_features = np.array([x for x in ft_features_])\n",
    "ft_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44849, 16)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_ftr_data = np.array([other_features(tweet) for tweet, _ in tweet_pairs])\n",
    "other_ftr_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44849, 216) (44849,)\n"
     ]
    }
   ],
   "source": [
    "X_data = np.concatenate([\n",
    "    ft_features, \n",
    "    other_ftr_data, \n",
    "#     pos\n",
    "], axis=1)\n",
    "\n",
    "print(X_data.shape, y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_state = 1337"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params = {\n",
    "    'n_estimators': 128, \n",
    "    \"n_jobs\": -1,\n",
    "    'random_state': r_state,\n",
    "    'class_weight': \"balanced\",\n",
    "    'criterion': 'gini',\n",
    "    'max_depth': 32,\n",
    "    'max_features': 113,\n",
    "    'min_samples_leaf': 2,\n",
    "    'min_samples_split': 54,\n",
    "}\n",
    "\n",
    "nb_params = {\n",
    "    'alpha': 0.6836531055077686,\n",
    "    'binarize': 0.027689715150536642,\n",
    "    'fit_prior': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAccuracy: 0.11269170926872639\n",
      "\tF1: 0.10193283060329068\n",
      "\tAccuracy: 0.11128421989761851\n",
      "\tF1: 0.10203009633774549\n",
      "\tAccuracy: 0.11720142602495544\n",
      "\tF1: 0.13123838549707853\n",
      "\tAccuracy: 0.1323529411764706\n",
      "\tF1: 0.14218445417061035\n",
      "\tAccuracy: 0.14626532887402452\n",
      "\tF1: 0.14401338264046543\n",
      "\tAccuracy: 0.1629877369007804\n",
      "\tF1: 0.1295520063443928\n",
      "\tAccuracy: 0.15442981477348805\n",
      "\tF1: 0.15127712477894503\n",
      "\tAccuracy: 0.1134181736994865\n",
      "\tF1: 0.13532551924977937\n",
      "\tAccuracy: 0.08109919571045576\n",
      "\tF1: 0.11720975760158868\n",
      "\tAccuracy: 0.1153072625698324\n",
      "\tF1: 0.10071373038158765\n",
      "Accuracy: 0.12470378088958387\n",
      "F1: 0.1255477287605484\n"
     ]
    }
   ],
   "source": [
    "f1_accum = []\n",
    "accuracy_accum = []\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10, random_state=r_state)\n",
    "for train, test in skf.split(X_data, y_data):\n",
    "\n",
    "    X_train = X_data[train]\n",
    "    y_train = y_data[train]\n",
    "    \n",
    "    X_test = X_data[test]\n",
    "    y_test = y_data[test]\n",
    "\n",
    "    # train\n",
    "    fitted_model = RandomForestClassifier(**rf_params)\n",
    "#     fitted_model = BernoulliNB(**nb_params)\n",
    "    fitted_model.fit(X_train, y_train)\n",
    "\n",
    "    # Compute Precision-Recall \n",
    "    y_infer_local = fitted_model.predict(X_test)\n",
    "    local_f1 = f1_score(y_test, y_infer_local, average=\"macro\")\n",
    "    local_score = fitted_model.score(X_test, y_test)\n",
    "    print(\"\\tAccuracy:\", local_score)\n",
    "    print(\"\\tF1:\", local_f1)\n",
    "    \n",
    "    f1_accum.append(local_f1)\n",
    "    accuracy_accum.append(local_score)\n",
    "\n",
    "print(\"Accuracy:\", np.mean(accuracy_accum))\n",
    "print(\"F1:\", np.mean(f1_accum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "category_number_list = list(category_to_label.values())\n",
    "label_to_category = {j:i for i, j in category_to_label.items()}\n",
    "for positive_category in category_number_list:\n",
    "    local_y_data = [1 if y == positive_category else 0 for y in y_data]\n",
    "    \n",
    "#     fitted_model = RandomForestClassifier(**rf_params)\n",
    "    fitted_model = BernoulliNB(**nb_params)\n",
    "    fitted_model.fit(X_data, local_y_data)\n",
    "   \n",
    "    print(\"Label:\", label_to_category[positive_category])\n",
    "    print(\"Score:\", fitted_model.score(X_data, local_y_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_search(X_data, y_data, clf, param_dist, n_iter_search=20, r_state=1337):\n",
    "    # run randomized search\n",
    "    random_search = RandomizedSearchCV(clf, \n",
    "                                       param_distributions=param_dist,\n",
    "                                       n_iter=n_iter_search,\n",
    "                                       cv=10,\n",
    "                                       scoring=\"f1_macro\",\n",
    "                                       random_state=r_state,\n",
    "                                       verbose=2,\n",
    "                                       n_jobs=-1,\n",
    "                                      )\n",
    "    \n",
    "    random_search.fit(X_data, y_data)\n",
    "\n",
    "    return (random_search.best_score_, random_search.best_params_)\n",
    "\n",
    "def model_eval_rf(X_data, y_data, n_iter_search=100, r_state=1337):\n",
    "\n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=100, class_weight=\"balanced\", random_state=r_state)\n",
    "    \n",
    "    # specify parameters and distributions to sample from\n",
    "    param_dist = {\n",
    "        \"max_depth\": scipy.stats.randint(2, 8),\n",
    "        \"max_features\": scipy.stats.randint(2, min(128, X_data.shape[1])),\n",
    "        \"min_samples_split\": scipy.stats.randint(2, 512),\n",
    "        \"min_samples_leaf\": scipy.stats.randint(2, 512),\n",
    "#         \"criterion\": [\"gini\", \"entropy\"],\n",
    "    }\n",
    "    \n",
    "    return random_search(X_data, y_data, clf, param_dist, n_iter_search=n_iter_search, r_state=r_state)\n",
    "\n",
    "def model_eval_nb(X_data, y_data, n_iter_search=100, r_state=1337):\n",
    "\n",
    "    clf = BernoulliNB()\n",
    "    \n",
    "    # specify parameters and distributions to sample from\n",
    "    param_dist = {\n",
    "        \"alpha\": scipy.stats.uniform(),\n",
    "        \"binarize\": scipy.stats.uniform(),\n",
    "        \"fit_prior\": [True, False],\n",
    "    }\n",
    "    \n",
    "    return random_search(X_data, y_data, clf, param_dist, n_iter_search=n_iter_search, r_state=r_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "search_results = model_eval_nb(X_data, y_data, n_iter_search=128)\n",
    "search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.12229702240170447,\n",
       " {'alpha': 0.6836531055077686,\n",
       "  'binarize': 0.027689715150536642,\n",
       "  'fit_prior': True})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smapper(x):\n",
    "    if ( x < 0.75 ):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "y_data_regress = np.array([smapper(tweet_priority_map[np.int64(tp[0][\"id\"])][\"score\"]) \n",
    "                           for tp in tweet_pairs])\n",
    "y_data_weights = np.array([tweet_priority_map[np.int64(tp[0][\"id\"])][\"weight\"] \n",
    "                           for tp in tweet_pairs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_priority_params = {\n",
    "    'random_state': r_state,\n",
    "    'class_weight': 'balanced',\n",
    "    'n_estimators': 128, \n",
    "    'n_jobs': -1,\n",
    "    'max_depth': 50,\n",
    "    'max_features': 14,\n",
    "    'min_samples_leaf': 33,\n",
    "    'min_samples_split': 96,\n",
    "}\n",
    "\n",
    "nb_priority_params = {\n",
    "    'alpha': 0.05134305647695325,\n",
    "    'binarize': 0.045909955637688404,\n",
    "    'fit_prior': True,\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFitting...\n",
      "\t 0.8642443156486848\n",
      "\tFitting...\n",
      "\t 0.9333333333333333\n",
      "\tFitting...\n",
      "\t 0.8849498327759198\n",
      "\tFitting...\n",
      "\t 0.9549609810479376\n",
      "\tFitting...\n",
      "\t 0.9369007803790412\n",
      "\tFitting...\n",
      "\t 0.9531772575250836\n",
      "\tFitting...\n",
      "\t 0.9306577480490524\n",
      "\tFitting...\n",
      "\t 0.9088071348940914\n",
      "\tFitting...\n",
      "\t 0.9471454058876003\n",
      "\tFitting...\n",
      "\t 0.9092328278322926\n",
      "Score (R^2): 0.9223409617373038\n",
      "Accuracy: 0.9223409617373038\n",
      "F1: 0.8558994805140546\n"
     ]
    }
   ],
   "source": [
    "score_accum = []\n",
    "f1_accum = []\n",
    "accuracy_accum = []\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10, random_state=r_state)\n",
    "for train, test in skf.split(X_data, y_data_regress):\n",
    "\n",
    "    X_train = X_data[train]\n",
    "    y_train = y_data_regress[train]\n",
    "    y_weight = y_data_weights[train]\n",
    "    \n",
    "    X_test = X_data[test]\n",
    "    y_test = y_data_regress[test]\n",
    "\n",
    "    # train\n",
    "    print(\"\\tFitting...\")\n",
    "#     fitted_model = sklearn.linear_model.LinearRegression(n_jobs=4)\n",
    "#     fitted_model = sklearn.tree.DecisionTreeRegressor(random_state=r_state, max_depth=256)\n",
    "#     fitted_model = BernoulliNB(**nb_priority_params)\n",
    "    fitted_model = RandomForestClassifier(**rf_priority_params)\n",
    "    fitted_model.fit(X_train, y_train, y_weight)\n",
    "\n",
    "    # Compute score metrics\n",
    "    r2_score = fitted_model.score(X_test, y_test)\n",
    "    score_accum.append(r2_score)\n",
    "    \n",
    "    # Compute Precision-Recall \n",
    "    y_infer_local = fitted_model.predict(X_test)\n",
    "    f1_accum.append(f1_score(y_test, y_infer_local, average=\"macro\"))\n",
    "    \n",
    "    accuracy_accum.append(fitted_model.score(X_test, y_test))\n",
    "    \n",
    "    print(\"\\t\", r2_score)\n",
    "\n",
    "print(\"Score (R^2):\", np.mean(score_accum))\n",
    "print(\"Accuracy:\", np.mean(accuracy_accum))\n",
    "print(\"F1:\", np.mean(f1_accum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found\n",
      "Found\n",
      "Found\n",
      "(9503, 216)\n"
     ]
    }
   ],
   "source": [
    "test_tweets = []\n",
    "with open(\"../data/2019-testing.json\", \"r\") as in_file:\n",
    "    for line in in_file:\n",
    "        tweet = json.loads(line)\n",
    "        test_tweets.append(tweet)\n",
    "        \n",
    "X_test_ft_ = [vectorize(s) for s in [t[\"text\"] for t in test_tweets]]\n",
    "X_test_ft_ = np.array([x for x in X_test_ft_])\n",
    "\n",
    "X_test_other = np.array([other_features(tweet) for tweet in test_tweets])\n",
    "\n",
    "X_test_data = np.concatenate([\n",
    "    X_test_ft_, \n",
    "    X_test_other, \n",
    "], axis=1)\n",
    "\n",
    "print(X_test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "            criterion='gini', max_depth=32, max_features=113,\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=2,\n",
       "            min_samples_split=54, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=128, n_jobs=-1, oob_score=False,\n",
       "            random_state=1337, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitted_model = BernoulliNB(**nb_params)\n",
    "fitted_model = RandomForestClassifier(**rf_params)\n",
    "fitted_model.fit(X_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_labels = fitted_model.predict(X_test_data)\n",
    "\n",
    "labeled_test_data = list(zip([t[\"id\"] for t in test_tweets], y_test_labels))\n",
    "\n",
    "id_to_cat_map = {y:x for x,y in category_to_label.items()}\n",
    "\n",
    "df = pd.DataFrame([\n",
    "    {\"tweet_id\":np.int64(tup[0]), \"label\": id_to_cat_map[tup[1]]} for tup in labeled_test_data],\n",
    ")\n",
    "\n",
    "df.to_csv(\"trec2019_test_results_run_fasttext.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Advice</th>\n",
       "      <td>1014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CleanUp</th>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ContinuingNews</th>\n",
       "      <td>321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Discussion</th>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Donations</th>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EmergingThreats</th>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Factoid</th>\n",
       "      <td>487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FirstPartyObservation</th>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GoodsServices</th>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hashtags</th>\n",
       "      <td>803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>InformationWanted</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Irrelevant</th>\n",
       "      <td>1223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KnownAlready</th>\n",
       "      <td>272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MovePeople</th>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultimediaShare</th>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Official</th>\n",
       "      <td>337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PastNews</th>\n",
       "      <td>1680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SearchAndRescue</th>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentiment</th>\n",
       "      <td>1042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ServiceAvailable</th>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SignificantEventChange</th>\n",
       "      <td>356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ThirdPartyObservation</th>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unknown</th>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Volunteer</th>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weather</th>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        tweet_id\n",
       "label                           \n",
       "Advice                      1014\n",
       "CleanUp                       29\n",
       "ContinuingNews               321\n",
       "Discussion                   332\n",
       "Donations                    215\n",
       "EmergingThreats              212\n",
       "Factoid                      487\n",
       "FirstPartyObservation        205\n",
       "GoodsServices                 17\n",
       "Hashtags                     803\n",
       "InformationWanted             15\n",
       "Irrelevant                  1223\n",
       "KnownAlready                 272\n",
       "MovePeople                   115\n",
       "MultimediaShare              235\n",
       "Official                     337\n",
       "PastNews                    1680\n",
       "SearchAndRescue               71\n",
       "Sentiment                   1042\n",
       "ServiceAvailable             151\n",
       "SignificantEventChange       356\n",
       "ThirdPartyObservation         86\n",
       "Unknown                       22\n",
       "Volunteer                     35\n",
       "Weather                      228"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"label\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1100622467733901312    3\n",
       "1101050969494880256    3\n",
       "1100698297155633153    3\n",
       "1100807416327950343    3\n",
       "1100793945758490624    3\n",
       "1101035588692701185    3\n",
       "1101144348144660483    3\n",
       "1100698558691532800    3\n",
       "1100865703412928518    3\n",
       "1100448617926668288    3\n",
       "1100814740408938497    3\n",
       "1101181539071782914    3\n",
       "1101396685098487808    2\n",
       "1101287326767108098    2\n",
       "1101204327262232576    2\n",
       "1101520730225815552    2\n",
       "1101028104473071616    2\n",
       "1100823316992950275    2\n",
       "1100915273362800652    2\n",
       "1100741253556252672    2\n",
       "1101104831647940614    2\n",
       "1101150862926405632    2\n",
       "1101826026299183104    2\n",
       "1102315424300089344    2\n",
       "1101185478664118272    2\n",
       "1102465381183176704    2\n",
       "1100957207225499648    2\n",
       "1101696298393194497    2\n",
       "1101626660280066048    2\n",
       "1102115769423405056    2\n",
       "                      ..\n",
       "751241406820323328     1\n",
       "751241399123775488     1\n",
       "751241388960911360     1\n",
       "751241382619123712     1\n",
       "751241354873704448     1\n",
       "751241632922505216     1\n",
       "727632633912418304     1\n",
       "751241326528790530     1\n",
       "751241334728429568     1\n",
       "1041336471691649024    1\n",
       "751241317578002432     1\n",
       "751241303006908416     1\n",
       "751241451296608256     1\n",
       "1101994466859114496    1\n",
       "751241470934257664     1\n",
       "1040967200834764801    1\n",
       "392149782791073792     1\n",
       "390390578438868992     1\n",
       "390046200897871874     1\n",
       "751241522029465600     1\n",
       "751241528081846272     1\n",
       "751242616625324032     1\n",
       "751241544426852352     1\n",
       "727639350410764288     1\n",
       "751241575024431104     1\n",
       "751241592434987008     1\n",
       "751241601133924352     1\n",
       "751241616417886208     1\n",
       "751241620452806656     1\n",
       "727626929327591424     1\n",
       "Name: tweet_id, Length: 9308, dtype: int64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"tweet_id\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "            criterion='gini', max_depth=50, max_features=14,\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=33,\n",
       "            min_samples_split=96, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=128, n_jobs=-1, oob_score=False,\n",
       "            random_state=1337, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted_pri_model = RandomForestClassifier(**rf_priority_params)\n",
    "fitted_pri_model.fit(X_data, y_data_regress)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_labels = fitted_pri_model.predict(X_test_data)\n",
    "\n",
    "labeled_test_data = list(zip([t[\"id\"] for t in test_tweets], y_test_labels))\n",
    "\n",
    "df = pd.DataFrame([{\"tweet_id\":tup[0], \"priority\": tup[1]} for tup in labeled_test_data])\n",
    "\n",
    "df.to_csv(\"trec2019_test_results_priority_run_fasttext.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    8608\n",
       "1     895\n",
       "Name: priority, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"priority\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Multi-Label Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_pairs = []\n",
    "y_data = []\n",
    "\n",
    "indexer = {c:i for i, c in enumerate(list(category_to_label.keys()))}\n",
    "indexer_inv = {i:c for c,i in indexer.items()}\n",
    "for tweet_id, categories in category_df.groupby(\"tweet_id\"):\n",
    "    \n",
    "    tup = (\n",
    "        tweet_id_map[np.int64(tweet_id)], \n",
    "        [indexer[category] for category in categories[\"category\"]]\n",
    "    )\n",
    "    tweet_pairs.append(tup)\n",
    "\n",
    "tweet_texts = [tp[0][\"text\"] for tp in tweet_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data_ = [tp[1] for tp in tweet_pairs]\n",
    "\n",
    "def one_hot_y(y_list):\n",
    "    encoded = [0] * len(indexer)\n",
    "    for y in y_list:\n",
    "        encoded[y] = 1\n",
    "    return encoded\n",
    "\n",
    "y_data = np.array([one_hot_y(y) for y in y_data_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19046, 200)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_features_ = [vectorize(s) for s in tweet_texts]\n",
    "ft_features = np.array([x for x in ft_features_])\n",
    "ft_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19046, 16)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_ftr_data = np.array([other_features(tweet) for tweet, _ in tweet_pairs])\n",
    "other_ftr_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19046, 216) (19046, 25)\n"
     ]
    }
   ],
   "source": [
    "X_data = np.concatenate([\n",
    "    ft_features, \n",
    "    other_ftr_data, \n",
    "], axis=1)\n",
    "\n",
    "print(X_data.shape, y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rf_params = {\n",
    "    'n_estimators': 128, \n",
    "    \"n_jobs\": -1,\n",
    "    'random_state': r_state,\n",
    "    'class_weight': \"balanced\",\n",
    "    'criterion': 'gini',\n",
    "    'max_depth': 32,\n",
    "    'max_features': 113,\n",
    "    'min_samples_leaf': 2,\n",
    "    'min_samples_split': 54,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_accum = []\n",
    "accuracy_accum = []\n",
    "\n",
    "skf = KFold(n_splits=10, random_state=r_state)\n",
    "for train, test in skf.split(X_data, y_data):\n",
    "\n",
    "    X_train = X_data[train]    \n",
    "    X_test = X_data[test]\n",
    "    \n",
    "    y_train = y_data[train]\n",
    "    y_test = y_data[test]\n",
    "\n",
    "\n",
    "    # train\n",
    "    fitted_model = RandomForestClassifier(**rf_params)\n",
    "    fitted_model.fit(X_train, y_train)\n",
    "\n",
    "    # Compute Precision-Recall \n",
    "    y_infer_local = fitted_model.predict(X_test)\n",
    "    local_f1 = f1_score(y_test, y_infer_local, average=\"weighted\")\n",
    "    local_score = fitted_model.score(X_test, y_test)\n",
    "    print(\"\\tAccuracy:\", local_score)\n",
    "    print(\"\\tF1:\", local_f1)\n",
    "    \n",
    "    f1_accum.append(local_f1)\n",
    "    accuracy_accum.append(local_score)\n",
    "\n",
    "print(\"Accuracy:\", np.mean(accuracy_accum))\n",
    "print(\"F1:\", np.mean(f1_accum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = fitted_model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = sklearn.multiclass.OneVsRestClassifier(RandomForestClassifier(**rf_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.preprocessing.normalize(results[0].reshape(-1,1), norm=\"l1\", axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = KFold(n_splits=10, random_state=r_state)\n",
    "for train, test in skf.split(X_data, y_data):\n",
    "\n",
    "    X_train = X_data[train]    \n",
    "    X_test = X_data[test]\n",
    "    \n",
    "    y_train = y_data[train]\n",
    "    y_test = y_data[test]\n",
    "\n",
    "\n",
    "    # train\n",
    "    fitted_model = sklearn.multiclass.OneVsRestClassifier(RandomForestClassifier(**rf_params))\n",
    "    fitted_model.fit(X_train, y_train)\n",
    "\n",
    "    # Compute Precision-Recall \n",
    "    y_infer_local = fitted_model.predict(X_test)\n",
    "    local_f1 = f1_score(y_test, y_infer_local, average=\"weighted\")\n",
    "    local_score = fitted_model.score(X_test, y_test)\n",
    "    print(\"\\tAccuracy:\", local_score)\n",
    "    print(\"\\tF1:\", local_f1)\n",
    "    \n",
    "    f1_accum.append(local_f1)\n",
    "    accuracy_accum.append(local_score)\n",
    "\n",
    "print(\"Accuracy:\", np.mean(accuracy_accum))\n",
    "print(\"F1:\", np.mean(f1_accum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prs = fitted_model.predict(X_test)\n",
    "prs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, x in enumerate(prs.sum(axis=0)):\n",
    "    print(i, indexer_inv[i], x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "            criterion='gini', max_depth=32, max_features=113,\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=2,\n",
       "            min_samples_split=54, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=128, n_jobs=-1, oob_score=False,\n",
       "            random_state=1337, verbose=0, warm_start=False),\n",
       "          n_jobs=None)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = sklearn.multiclass.OneVsRestClassifier(\n",
    "    RandomForestClassifier(**rf_params)\n",
    "#     BernoulliNB(**nb_params)\n",
    ")\n",
    "clf.fit(X_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_labels = clf.predict(X_test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rows = []\n",
    "for tid, row_labels in zip([t[\"id\"] for t in test_tweets], y_test_labels):\n",
    "    \n",
    "    row = [tid] + row_labels.tolist()\n",
    "    all_rows.append(row)\n",
    "\n",
    "\n",
    "id_to_cat_map = {y:x for x,y in category_to_label.items()}\n",
    "\n",
    "df = pd.DataFrame(all_rows, columns=[\"tweet_id\"] + [indexer_inv[i] for i in range(len(indexer_inv))])\n",
    "\n",
    "df.to_csv(\"trec2019_test_results_run_fasttext_multi.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
